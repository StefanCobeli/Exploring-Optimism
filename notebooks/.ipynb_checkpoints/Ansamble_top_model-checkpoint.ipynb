{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ansamble prediction from logits model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data best logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING 0\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "opt_df_train_al = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_train_Acc_0.9163306451612904.csv\")\n",
    "opt_df_test_al  = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_test_Acc_0.8203125.csv\")\n",
    "opt_df_dev_al   = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_val_Acc_0.83203125.csv\")\n",
    "\n",
    "opt_df_train_bb = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_train_Acc_0.9163306451612904.csv\")\n",
    "opt_df_test_bb  = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_test_Acc_0.8203125.csv\")\n",
    "opt_df_dev_bb   = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_val_Acc_0.83203125.csv\")\n",
    "\n",
    "opt_df_train_bl = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_train_Acc_0.9163306451612904.csv\")\n",
    "opt_df_test_bl  = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_test_Acc_0.8203125.csv\")\n",
    "opt_df_dev_bl   = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_val_Acc_0.83203125.csv\")\n",
    "\n",
    "opt_df_train_rb = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_train_Acc_0.9163306451612904.csv\")\n",
    "opt_df_test_rb  = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_test_Acc_0.8203125.csv\")\n",
    "opt_df_dev_rb   = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_val_Acc_0.83203125.csv\")\n",
    "\n",
    "opt_df_train_xl = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_train_Acc_0.9163306451612904.csv\")\n",
    "opt_df_test_xl  = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_test_Acc_0.8203125.csv\")\n",
    "opt_df_dev_xl   = pd.read_csv(\"../../data/optimism-twitter-data/processed/Best_Logits/Logits_OPT_Hate_albert-base-v2_set0_it_3of5_val_Acc_0.83203125.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING 0\n",
    "opt_df_train1 = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set0_M1_Bert_Large_M2_XLNet_Base_train.csv\")\n",
    "opt_df_test1  = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set0_M1_Bert_Large_M2_XLNet_Base_test.csv\")\n",
    "opt_df_dev1   = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set0_M1_Bert_Large_M2_XLNet_Base_validation.csv\")\n",
    "\n",
    "opt_df_train2 = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set0_M1_Bert_Base_M2_Roberta_Base_train.csv\")\n",
    "opt_df_test2  = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set0_M1_Bert_Base_M2_Roberta_Base_test.csv\")\n",
    "opt_df_dev2   = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set0_M1_Bert_Base_M2_Roberta_Base_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING 1M1\n",
    "opt_df_train1 = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set1M1_M1_Bert_Base_M2_XLNet_Base_train.csv\")\n",
    "opt_df_test1  = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set1M1_M1_Bert_Base_M2_XLNet_Base_test.csv\")\n",
    "opt_df_dev1   = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set1M1_M1_Bert_Base_M2_XLNet_Base_validation.csv\")\n",
    "\n",
    "opt_df_train2 = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set1M1_M1_Bert_Large_M2_Roberta_Base_train.csv\")\n",
    "opt_df_test2  = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set1M1_M1_Bert_Large_M2_Roberta_Base_test.csv\")\n",
    "opt_df_dev2   = pd.read_csv(\"../../data/optimism-twitter-data/processed/optimism_Ansamble_set1M1_M1_Bert_Large_M2_Roberta_Base_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>AverageAnnotation</th>\n",
       "      <th>Original ID</th>\n",
       "      <th>M1:Bert_Large_logit0</th>\n",
       "      <th>M1:Bert_Large_logit1</th>\n",
       "      <th>M2:XLNet_Base_logit0</th>\n",
       "      <th>M2:XLNet_Base_logit1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@runge_kelly rest is good!! if you like tea an...</td>\n",
       "      <td>TheAnglophiler</td>\n",
       "      <td>1.25</td>\n",
       "      <td>220</td>\n",
       "      <td>-5.205334</td>\n",
       "      <td>4.681307</td>\n",
       "      <td>-3.560899</td>\n",
       "      <td>3.339703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dr. budiani-saberi is a medical anthropologist...</td>\n",
       "      <td>JessiKersi</td>\n",
       "      <td>0.00</td>\n",
       "      <td>916</td>\n",
       "      <td>2.511252</td>\n",
       "      <td>-2.607783</td>\n",
       "      <td>2.411943</td>\n",
       "      <td>-2.060660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i want city to win. chelsea going up too far w...</td>\n",
       "      <td>Ajinkyaworld</td>\n",
       "      <td>1.50</td>\n",
       "      <td>321</td>\n",
       "      <td>-5.192333</td>\n",
       "      <td>4.742418</td>\n",
       "      <td>-2.866381</td>\n",
       "      <td>3.463603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>you're annoying the piss out of me</td>\n",
       "      <td>TehhKota</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>2127</td>\n",
       "      <td>4.497906</td>\n",
       "      <td>-4.169426</td>\n",
       "      <td>4.160317</td>\n",
       "      <td>-3.558687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@arapahoe_basin looking good for tomorrow's sk...</td>\n",
       "      <td>tomfricke</td>\n",
       "      <td>0.60</td>\n",
       "      <td>5643</td>\n",
       "      <td>-5.039854</td>\n",
       "      <td>4.488212</td>\n",
       "      <td>-3.775995</td>\n",
       "      <td>3.775713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Tweet  \\\n",
       "0           0  @runge_kelly rest is good!! if you like tea an...   \n",
       "1           1  dr. budiani-saberi is a medical anthropologist...   \n",
       "2           2  i want city to win. chelsea going up too far w...   \n",
       "3           3                 you're annoying the piss out of me   \n",
       "4           4  @arapahoe_basin looking good for tomorrow's sk...   \n",
       "\n",
       "         Username  AverageAnnotation  Original ID  M1:Bert_Large_logit0  \\\n",
       "0  TheAnglophiler               1.25          220             -5.205334   \n",
       "1      JessiKersi               0.00          916              2.511252   \n",
       "2    Ajinkyaworld               1.50          321             -5.192333   \n",
       "3        TehhKota              -1.80         2127              4.497906   \n",
       "4       tomfricke               0.60         5643             -5.039854   \n",
       "\n",
       "   M1:Bert_Large_logit1  M2:XLNet_Base_logit0  M2:XLNet_Base_logit1  \n",
       "0              4.681307             -3.560899              3.339703  \n",
       "1             -2.607783              2.411943             -2.060660  \n",
       "2              4.742418             -2.866381              3.463603  \n",
       "3             -4.169426              4.160317             -3.558687  \n",
       "4              4.488212             -3.775995              3.775713  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #X_train\n",
    "# np.vstack([opt_df_train[\"M1:Bert_Large_logit0\"]\\\n",
    "#           , opt_df_train[\"M1:Bert_Large_logit1\"]\\\n",
    "#           , opt_df_train[\"M2:XLNet_Base_logit0\"]\\\n",
    "#           , opt_df_train[\"M2:XLNet_Base_logit1\"]])#.shape\n",
    "# #y_train\n",
    "# np.where(opt_df_train[\"AverageAnnotation\"]<=0, 0, 1)#.shape\n",
    "\n",
    "opt_df_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: \n",
      "Train:      0.9884, \n",
      "Validation: 0.8255, \n",
      "Test:       0.8333;\n",
      "\n",
      "SVM: \n",
      "Train:      0.9879, \n",
      "Validation: 0.8320, \n",
      "Test:       0.8320;   \n",
      "\n",
      "MLP: \n",
      "Train:      0.9886, \n",
      "Validation: 0.8268, \n",
      "Test:       0.8307;   \n",
      "\n",
      "KNN: \n",
      "Train:      0.9879, \n",
      "Validation: 0.8229, \n",
      "Test:       0.8333.   \n"
     ]
    }
   ],
   "source": [
    "#Setting 0\n",
    "#XGBoost\n",
    "#https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn          import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# X, y = make_hastie_10_2(random_state=0)\n",
    "# X_train, X_test = X[:2000], X[2000:]\n",
    "# y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "mock_config  = list(range(8))\n",
    "mock_config  = list(max_mc)# [] \n",
    "mock_config  = [6, 7]# [] \n",
    "mock_config  = [6]# [] \n",
    "mock_config  = [7]# [] \n",
    "\n",
    "X_train = np.vstack([\\\n",
    "                     opt_df_train1[\"M1:Bert_Large_logit0\"]\\\n",
    "          , opt_df_train1[\"M1:Bert_Large_logit1\"]\\\n",
    "          , opt_df_train1[\"M2:XLNet_Base_logit0\"]\\\n",
    "          , opt_df_train1[\"M2:XLNet_Base_logit1\"]\\\n",
    "          , opt_df_train2[\"M1:Bert_Base_logit0\"]\\\n",
    "          , opt_df_train2[\"M1:Bert_Base_logit1\"]\\\n",
    "          , opt_df_train2[\"M2:Roberta_Base_logit0\"]\\\n",
    "          , opt_df_train2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                    ]).T\n",
    "X_train = X_train.T[mock_config].T\n",
    "y_train = np.where(opt_df_train1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "X_dev = np.vstack([\\\n",
    "                   opt_df_dev1[\"M1:Bert_Large_logit0\"]\\\n",
    "          , opt_df_dev1[\"M1:Bert_Large_logit1\"]\\\n",
    "          , opt_df_dev1[\"M2:XLNet_Base_logit0\"]\\\n",
    "          , opt_df_dev1[\"M2:XLNet_Base_logit1\"]\\\n",
    "          , opt_df_dev2[\"M1:Bert_Base_logit0\"]\\\n",
    "          , opt_df_dev2[\"M1:Bert_Base_logit1\"]\\\n",
    "          , opt_df_dev2[\"M2:Roberta_Base_logit0\"]\\\n",
    "          , opt_df_dev2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                  ]).T\n",
    "X_dev = X_dev.T[mock_config].T\n",
    "y_dev = np.where(opt_df_dev1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "X_test = np.vstack([\\\n",
    "                    opt_df_test1[\"M1:Bert_Large_logit0\"]\\\n",
    "          , opt_df_test1[\"M1:Bert_Large_logit1\"]\\\n",
    "          , opt_df_test1[\"M2:XLNet_Base_logit0\"]\\\n",
    "          , opt_df_test1[\"M2:XLNet_Base_logit1\"]\\\n",
    "          , opt_df_test2[\"M1:Bert_Base_logit0\"]\\\n",
    "          , opt_df_test2[\"M1:Bert_Base_logit1\"]\\\n",
    "          , opt_df_test2[\"M2:Roberta_Base_logit0\"]\\\n",
    "          , opt_df_test2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                   ]).T\n",
    "X_test = X_test.T[mock_config].T\n",
    "y_test = np.where(opt_df_test1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "clf1  = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf2  = svm.NuSVC(gamma='auto').fit(X_train, y_train)#.fit(X, Y)\n",
    "clf3  = MLPClassifier(hidden_layer_sizes=(32, 16, 4), random_state=1).fit(X_train, y_train)\n",
    "clf4  = KNeighborsClassifier(n_neighbors=15, algorithm=\"kd_tree\").fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;\"\\\n",
    "      %(clf1.score(X_train, y_train).round(4)\\\n",
    "      , clf1.score(X_dev, y_dev).round(4)\\\n",
    "      , clf1.score(X_test, y_test).round(4)))\n",
    "print()\n",
    "print(\"SVM: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "      %(clf2.score(X_train, y_train).round(4)\\\n",
    "        , clf2.score(X_dev, y_dev).round(4)\\\n",
    "        , clf2.score(X_test, y_test).round(4)))\n",
    "print()\n",
    "print(\"MLP: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "      %(clf3.score(X_train, y_train).round(4)\\\n",
    "        , clf3.score(X_dev, y_dev).round(4)\\\n",
    "        , clf3.score(X_test, y_test).round(4)))\n",
    "print()\n",
    "print(\"KNN: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f.   \"\\\n",
    "      %(clf4.score(X_train, y_train).round(4)\\\n",
    "      , clf4.score(X_dev, y_dev).round(4)\\\n",
    "      , clf4.score(X_test, y_test).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 5939), (5, 8), (0, 1, 5, 6, 7))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.T[[0, 1, 5, 6, 7]].shape, X_train[list(max_mc)].shape, max_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 5939), (4, 768))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: \n",
      "Train:      1.0000, \n",
      "Validation: 0.9736, \n",
      "Test:       0.9543;\n",
      "\n",
      "SVM: \n",
      "Train:      0.9980, \n",
      "Validation: 0.9543, \n",
      "Test:       0.9231;   \n",
      "\n",
      "MLP: \n",
      "Train:      1.0000, \n",
      "Validation: 0.9663, \n",
      "Test:       0.9495;   \n",
      "\n",
      "KNN: \n",
      "Train:      1.0000, \n",
      "Validation: 0.9663, \n",
      "Test:       0.9495.   \n"
     ]
    }
   ],
   "source": [
    "#Setting 1M1\n",
    "#XGBoost\n",
    "#https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn          import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# X, y = make_hastie_10_2(random_state=0)\n",
    "# X_train, X_test = X[:2000], X[2000:]\n",
    "# y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "X_train = np.vstack([\\\n",
    "#                      opt_df_train2[\"M1:Bert_Large_logit0\"]\\\n",
    "#           , opt_df_train2[\"M1:Bert_Large_logit1\"]\\\n",
    "           opt_df_train1[\"M2:XLNet_Base_logit0\"]\\\n",
    "          , opt_df_train1[\"M2:XLNet_Base_logit1\"]\\\n",
    "          , opt_df_train1[\"M1:Bert_Base_logit0\"]\\\n",
    "          , opt_df_train1[\"M1:Bert_Base_logit1\"]\\\n",
    "#           , opt_df_train2[\"M2:Roberta_Base_logit0\"]\\\n",
    "#           , opt_df_train2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                    ]).T\n",
    "y_train = np.where(opt_df_train1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "X_dev = np.vstack([\\\n",
    "#                    opt_df_dev2[\"M1:Bert_Large_logit0\"]\\\n",
    "#           , opt_df_dev2[\"M1:Bert_Large_logit1\"]\\\n",
    "          opt_df_dev1[\"M2:XLNet_Base_logit0\"]\\\n",
    "          , opt_df_dev1[\"M2:XLNet_Base_logit1\"]\\\n",
    "          , opt_df_dev1[\"M1:Bert_Base_logit0\"]\\\n",
    "          , opt_df_dev1[\"M1:Bert_Base_logit1\"]\\\n",
    "#           , opt_df_dev2[\"M2:Roberta_Base_logit0\"]\\\n",
    "#           , opt_df_dev2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                  ]).T\n",
    "y_dev = np.where(opt_df_dev1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "X_test = np.vstack([\\\n",
    "#                     opt_df_test2[\"M1:Bert_Large_logit0\"]\\\n",
    "#           , opt_df_test2[\"M1:Bert_Large_logit1\"]\\\n",
    "           opt_df_test1[\"M2:XLNet_Base_logit0\"]\\\n",
    "          , opt_df_test1[\"M2:XLNet_Base_logit1\"]\\\n",
    "          , opt_df_test1[\"M1:Bert_Base_logit0\"]\\\n",
    "          , opt_df_test1[\"M1:Bert_Base_logit1\"]\\\n",
    "#           , opt_df_test2[\"M2:Roberta_Base_logit0\"]\\\n",
    "#           , opt_df_test2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                   ]).T\n",
    "y_test = np.where(opt_df_test1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "clf1  = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf2  = svm.NuSVC(gamma='auto').fit(X_train, y_train)#.fit(X, Y)\n",
    "clf3  = MLPClassifier(hidden_layer_sizes=(32, 16, 4), random_state=1).fit(X_train, y_train)\n",
    "clf4  = KNeighborsClassifier(n_neighbors=20).fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;\"\\\n",
    "      %(clf1.score(X_train, y_train).round(4)\\\n",
    "      , clf1.score(X_dev, y_dev).round(4)\\\n",
    "      , clf1.score(X_test, y_test).round(4)))\n",
    "print()\n",
    "print(\"SVM: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "      %(clf2.score(X_train, y_train).round(4)\\\n",
    "        , clf2.score(X_dev, y_dev).round(4)\\\n",
    "        , clf2.score(X_test, y_test).round(4)))\n",
    "print()\n",
    "print(\"MLP: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "      %(clf3.score(X_train, y_train).round(4)\\\n",
    "        , clf3.score(X_dev, y_dev).round(4)\\\n",
    "        , clf3.score(X_test, y_test).round(4)))\n",
    "print()\n",
    "print(\"KNN: \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f.   \"\\\n",
    "      %(clf4.score(X_train, y_train).round(4)\\\n",
    "      , clf4.score(X_dev, y_dev).round(4)\\\n",
    "      , clf4.score(X_test, y_test).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination of features/logits:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting 0\n",
    "#Features train:\n",
    "features_train = [\\\n",
    "                    opt_df_train1[\"M1:Bert_Large_logit0\"]\\\n",
    "                  , opt_df_train1[\"M1:Bert_Large_logit1\"]\\\n",
    "                  , opt_df_train1[\"M2:XLNet_Base_logit0\"]\\\n",
    "                  , opt_df_train1[\"M2:XLNet_Base_logit1\"]\\\n",
    "                  , opt_df_train2[\"M1:Bert_Base_logit0\"]\\\n",
    "                  , opt_df_train2[\"M1:Bert_Base_logit1\"]\\\n",
    "                  , opt_df_train2[\"M2:Roberta_Base_logit0\"]\\\n",
    "                  , opt_df_train2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                ]\n",
    "\n",
    "features_dev = [\\\n",
    "                   opt_df_dev1[\"M1:Bert_Large_logit0\"]\\\n",
    "                  , opt_df_dev1[\"M1:Bert_Large_logit1\"]\\\n",
    "                  , opt_df_dev1[\"M2:XLNet_Base_logit0\"]\\\n",
    "                  , opt_df_dev1[\"M2:XLNet_Base_logit1\"]\\\n",
    "                  , opt_df_dev2[\"M1:Bert_Base_logit0\"]\\\n",
    "                  , opt_df_dev2[\"M1:Bert_Base_logit1\"]\\\n",
    "                  , opt_df_dev2[\"M2:Roberta_Base_logit0\"]\\\n",
    "                  , opt_df_dev2[\"M2:Roberta_Base_logit1\"]\\\n",
    "              ]\n",
    "\n",
    "features_test = [\\\n",
    "                    opt_df_test1[\"M1:Bert_Large_logit0\"]\\\n",
    "                  , opt_df_test1[\"M1:Bert_Large_logit1\"]\\\n",
    "                  , opt_df_test1[\"M2:XLNet_Base_logit0\"]\\\n",
    "                  , opt_df_test1[\"M2:XLNet_Base_logit1\"]\\\n",
    "                  , opt_df_test2[\"M1:Bert_Base_logit0\"]\\\n",
    "                  , opt_df_test2[\"M1:Bert_Base_logit1\"]\\\n",
    "                  , opt_df_test2[\"M2:Roberta_Base_logit0\"]\\\n",
    "                  , opt_df_test2[\"M2:Roberta_Base_logit1\"]\\\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/nlp-tf2/lib/python3.6/site-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2fb09e68594e4da2264eadeeaa1ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost: (0, 1, 2, 3, 4, 5, 6, 7) \n",
      "Train:      1.0000, \n",
      "Validation: 0.8177, \n",
      "Test:       0.8268;\n",
      "\n",
      "SVM: (0, 1, 2, 3, 4, 5, 6, 7) \n",
      "Train:      0.9951, \n",
      "Validation: 0.8346, \n",
      "Test:       0.8190;   \n",
      "\n",
      "SVM: (0, 1, 4, 5, 6, 7) \n",
      "Train:      0.9961, \n",
      "Validation: 0.8411, \n",
      "Test:       0.8268;   \n",
      "\n",
      "SVM: (0, 1, 5, 6, 7) \n",
      "Train:      0.9955, \n",
      "Validation: 0.8438, \n",
      "Test:       0.8281;   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/nlp-tf2/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Setting 0\n",
    "#XGBoost\n",
    "#https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "from itertools import chain, combinations\n",
    "s = list(range(len(features_dev)))\n",
    "# len(list(chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))))\n",
    "model_configurations =  list(chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1)))[::-1]\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn          import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# X, y = make_hastie_10_2(random_state=0)\n",
    "# X_train, X_test = X[:2000], X[2000:]\n",
    "# y_train, y_test = y[:2000], y[2000:]\n",
    "max_mc = None\n",
    "max_acc_train  = 0\n",
    "max_acc_val    = 0\n",
    "max_acc_test   = 0\n",
    "\n",
    "for mc in tqdm(model_configurations):\n",
    "    config_train = [features_train[i] for i in mc]\n",
    "    config_val   = [features_dev[i] for i in mc]\n",
    "    config_test  = [features_test[i] for i in mc]\n",
    "    \n",
    "    X_train = np.vstack([config_train]).T\n",
    "    y_train = np.where(opt_df_train1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    X_dev = np.vstack([config_val]).T\n",
    "    y_dev = np.where(opt_df_dev1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    X_test = np.vstack([config_test]).T\n",
    "    y_test = np.where(opt_df_test1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    clf1  = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "    clf2  = svm.NuSVC(gamma='auto').fit(X_train, y_train)#.fit(X, Y)\n",
    "    clf3  = MLPClassifier(hidden_layer_sizes=(32, 16, 4), random_state=1).fit(X_train, y_train)\n",
    "    clf4  = KNeighborsClassifier(n_neighbors=20).fit(X_train, y_train)\n",
    "    \n",
    "    xgb_score = clf1.score(X_dev, y_dev)#.round(4)\n",
    "    svm_score = clf2.score(X_dev, y_dev)#.round(4)\n",
    "    mlp_score = clf3.score(X_dev, y_dev)\n",
    "    knn_score = clf4.score(X_dev, y_dev)\n",
    "    \n",
    "    if xgb_score > max_acc_val:\n",
    "        max_acc_val = xgb_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nXGBoost:\", max_mc\\\n",
    "              , \"\\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;\"\\\n",
    "              %(clf1.score(X_train, y_train).round(4)\\\n",
    "              , clf1.score(X_dev, y_dev).round(4)\\\n",
    "              , clf1.score(X_test, y_test).round(4)))\n",
    "    if svm_score > max_acc_val:\n",
    "        max_acc_val = svm_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nSVM:\" , max_mc\\\n",
    "              , \"\\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "              %(clf2.score(X_train, y_train).round(4)\\\n",
    "                , clf2.score(X_dev, y_dev).round(4)\\\n",
    "                , clf2.score(X_test, y_test).round(4)))\n",
    "    if mlp_score > max_acc_val:\n",
    "        max_acc_val = mlp_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nMLP:\", max_mc\\\n",
    "              , \" \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "              %(clf3.score(X_train, y_train).round(4)\\\n",
    "                , clf3.score(X_dev, y_dev).round(4)\\\n",
    "                , clf3.score(X_test, y_test).round(4)))\n",
    "    if knn_score > max_acc_val:\n",
    "        max_acc_val = knn_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nKNN:\", max_mc\\\n",
    "              , \" \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f.   \"\\\n",
    "              %(clf4.score(X_train, y_train).round(4)\\\n",
    "              , clf4.score(X_dev, y_dev).round(4)\\\n",
    "              , clf4.score(X_test, y_test).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/nlp-tf2/lib/python3.6/site-packages/ipykernel_launcher.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e086d13d8daf4d3c8f4326cc542702c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/nlp-tf2/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost: (0,) \n",
      "Train:      0.9985, \n",
      "Validation: 0.8112, \n",
      "Test:       0.8177;\n",
      "\n",
      "SVM: (0,) \n",
      "Train:      0.9987, \n",
      "Validation: 0.8190, \n",
      "Test:       0.8216;   \n",
      "\n",
      "SVM: (1,) \n",
      "Train:      0.9968, \n",
      "Validation: 0.8255, \n",
      "Test:       0.8229;   \n",
      "\n",
      "SVM: (6,) \n",
      "Train:      0.9879, \n",
      "Validation: 0.8320, \n",
      "Test:       0.8320;   \n",
      "\n",
      "SVM: (0, 6) \n",
      "Train:      0.9961, \n",
      "Validation: 0.8385, \n",
      "Test:       0.8294;   \n",
      "\n",
      "SVM: (0, 7) \n",
      "Train:      0.9961, \n",
      "Validation: 0.8411, \n",
      "Test:       0.8307;   \n",
      "\n",
      "SVM: (0, 4, 6) \n",
      "Train:      0.9961, \n",
      "Validation: 0.8424, \n",
      "Test:       0.8255;   \n",
      "\n",
      "SVM: (0, 1, 5, 6, 7) \n",
      "Train:      0.9955, \n",
      "Validation: 0.8438, \n",
      "Test:       0.8281;   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting 0\n",
    "#XGBoost\n",
    "#https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "from itertools import chain, combinations\n",
    "s = list(range(len(features_dev)))\n",
    "# len(list(chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))))\n",
    "model_configurations =  list(chain.from_iterable(combinations(s, r) \\\n",
    "                                 for r in range(1, len(s)+1)))#[::-1]\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn          import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# X, y = make_hastie_10_2(random_state=0)\n",
    "# X_train, X_test = X[:2000], X[2000:]\n",
    "# y_train, y_test = y[:2000], y[2000:]\n",
    "max_mc = None\n",
    "max_acc_train  = 0\n",
    "max_acc_val    = 0\n",
    "max_acc_test   = 0\n",
    "\n",
    "for mc in tqdm(model_configurations):\n",
    "    config_train = [features_train[i] for i in mc]\n",
    "    config_val   = [features_dev[i] for i in mc]\n",
    "    config_test  = [features_test[i] for i in mc]\n",
    "    \n",
    "    X_train = np.vstack([config_train]).T\n",
    "    y_train = np.where(opt_df_train1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    X_dev = np.vstack([config_val]).T\n",
    "    y_dev = np.where(opt_df_dev1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    X_test = np.vstack([config_test]).T\n",
    "    y_test = np.where(opt_df_test1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    clf1  = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "    clf2  = svm.NuSVC(gamma='auto').fit(X_train, y_train)#.fit(X, Y)\n",
    "    clf3  = MLPClassifier(hidden_layer_sizes=(32, 16, 4), random_state=1).fit(X_train, y_train)\n",
    "    clf4  = KNeighborsClassifier(n_neighbors=20).fit(X_train, y_train)\n",
    "    \n",
    "    xgb_score = clf1.score(X_dev, y_dev)#.round(4)\n",
    "    svm_score = clf2.score(X_dev, y_dev)#.round(4)\n",
    "    mlp_score = clf3.score(X_dev, y_dev)\n",
    "    knn_score = clf4.score(X_dev, y_dev)\n",
    "    \n",
    "    if xgb_score > max_acc_val:\n",
    "        max_acc_val = xgb_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nXGBoost:\", max_mc\\\n",
    "              , \"\\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;\"\\\n",
    "              %(clf1.score(X_train, y_train).round(4)\\\n",
    "              , clf1.score(X_dev, y_dev).round(4)\\\n",
    "              , clf1.score(X_test, y_test).round(4)))\n",
    "    if svm_score > max_acc_val:\n",
    "        max_acc_val = svm_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nSVM:\" , max_mc\\\n",
    "              , \"\\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "              %(clf2.score(X_train, y_train).round(4)\\\n",
    "                , clf2.score(X_dev, y_dev).round(4)\\\n",
    "                , clf2.score(X_test, y_test).round(4)))\n",
    "    if mlp_score > max_acc_val:\n",
    "        max_acc_val = mlp_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nMLP:\", max_mc\\\n",
    "              , \" \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "              %(clf3.score(X_train, y_train).round(4)\\\n",
    "                , clf3.score(X_dev, y_dev).round(4)\\\n",
    "                , clf3.score(X_test, y_test).round(4)))\n",
    "    if knn_score > max_acc_val:\n",
    "        max_acc_val = knn_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nKNN:\", max_mc\\\n",
    "              , \" \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f.   \"\\\n",
    "              %(clf4.score(X_train, y_train).round(4)\\\n",
    "              , clf4.score(X_dev, y_dev).round(4)\\\n",
    "              , clf4.score(X_test, y_test).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 1/-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting 1M1\n",
    "#Features train:\n",
    "features_train = [\\\n",
    "                    opt_df_train2[\"M1:Bert_Large_logit0\"]\\\n",
    "                  , opt_df_train2[\"M1:Bert_Large_logit1\"]\\\n",
    "                  , opt_df_train1[\"M2:XLNet_Base_logit0\"]\\\n",
    "                  , opt_df_train1[\"M2:XLNet_Base_logit1\"]\\\n",
    "                  , opt_df_train1[\"M1:Bert_Base_logit0\"]\\\n",
    "                  , opt_df_train1[\"M1:Bert_Base_logit1\"]\\\n",
    "                  , opt_df_train2[\"M2:Roberta_Base_logit0\"]\\\n",
    "                  , opt_df_train2[\"M2:Roberta_Base_logit1\"]\\\n",
    "                ]\n",
    "\n",
    "features_dev = [\\\n",
    "                   opt_df_dev2[\"M1:Bert_Large_logit0\"]\\\n",
    "                  , opt_df_dev2[\"M1:Bert_Large_logit1\"]\\\n",
    "                  , opt_df_dev1[\"M2:XLNet_Base_logit0\"]\\\n",
    "                  , opt_df_dev1[\"M2:XLNet_Base_logit1\"]\\\n",
    "                  , opt_df_dev1[\"M1:Bert_Base_logit0\"]\\\n",
    "                  , opt_df_dev1[\"M1:Bert_Base_logit1\"]\\\n",
    "                  , opt_df_dev2[\"M2:Roberta_Base_logit0\"]\\\n",
    "                  , opt_df_dev2[\"M2:Roberta_Base_logit1\"]\\\n",
    "              ]\n",
    "\n",
    "features_test = [\\\n",
    "                    opt_df_test2[\"M1:Bert_Large_logit0\"]\\\n",
    "                  , opt_df_test2[\"M1:Bert_Large_logit1\"]\\\n",
    "                  , opt_df_test1[\"M2:XLNet_Base_logit0\"]\\\n",
    "                  , opt_df_test1[\"M2:XLNet_Base_logit1\"]\\\n",
    "                  , opt_df_test1[\"M1:Bert_Base_logit0\"]\\\n",
    "                  , opt_df_test1[\"M1:Bert_Base_logit1\"]\\\n",
    "                  , opt_df_test2[\"M2:Roberta_Base_logit0\"]\\\n",
    "                  , opt_df_test2[\"M2:Roberta_Base_logit1\"]\\\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "s = list(range(len(features_dev)))\n",
    "len(list(chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))))\n",
    "model_configurations =  list(chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/nlp-tf2/lib/python3.6/site-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2843a676efe74156b2f45899f26d8007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost: (0,) \n",
      "Train:      0.9980, \n",
      "Validation: 0.9399, \n",
      "Test:       0.9519;\n",
      "\n",
      "SVM: (0,) \n",
      "Train:      0.9990, \n",
      "Validation: 0.9519, \n",
      "Test:       0.9543;   \n",
      "\n",
      "MLP: (0,)  \n",
      "Train:      0.9997, \n",
      "Validation: 0.9615, \n",
      "Test:       0.9495;   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/nlp-tf2/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost: (2,) \n",
      "Train:      0.9973, \n",
      "Validation: 0.9736, \n",
      "Test:       0.9447;\n",
      "\n",
      "KNN: (2,)  \n",
      "Train:      0.9977, \n",
      "Validation: 0.9760, \n",
      "Test:       0.9423.   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/envs/nlp-tf2/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost: (2, 4) \n",
      "Train:      1.0000, \n",
      "Validation: 0.9784, \n",
      "Test:       0.9567;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting 1M1\n",
    "#XGBoost\n",
    "#https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn          import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# X, y = make_hastie_10_2(random_state=0)\n",
    "# X_train, X_test = X[:2000], X[2000:]\n",
    "# y_train, y_test = y[:2000], y[2000:]\n",
    "max_mc = None\n",
    "max_acc_train  = 0\n",
    "max_acc_val    = 0\n",
    "max_acc_test   = 0\n",
    "\n",
    "for mc in tqdm(model_configurations):\n",
    "    config_train = [features_train[i] for i in mc]\n",
    "    config_val   = [features_dev[i] for i in mc]\n",
    "    config_test  = [features_test[i] for i in mc]\n",
    "    \n",
    "    X_train = np.vstack([config_train]).T\n",
    "    y_train = np.where(opt_df_train1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    X_dev = np.vstack([config_val]).T\n",
    "    y_dev = np.where(opt_df_dev1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    X_test = np.vstack([config_test]).T\n",
    "    y_test = np.where(opt_df_test1[\"AverageAnnotation\"]<=0, 0, 1)\n",
    "\n",
    "    clf1  = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "    clf2  = svm.NuSVC(gamma='auto').fit(X_train, y_train)#.fit(X, Y)\n",
    "    clf3  = MLPClassifier(hidden_layer_sizes=(32, 16, 4), random_state=1).fit(X_train, y_train)\n",
    "    clf4  = KNeighborsClassifier(n_neighbors=20).fit(X_train, y_train)\n",
    "    \n",
    "    xgb_score = clf1.score(X_dev, y_dev)#.round(4)\n",
    "    svm_score = clf2.score(X_dev, y_dev)#.round(4)\n",
    "    mlp_score = clf3.score(X_dev, y_dev)\n",
    "    knn_score = clf4.score(X_dev, y_dev)\n",
    "    \n",
    "    if xgb_score > max_acc_val:\n",
    "        max_acc_val = xgb_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nXGBoost:\", max_mc\\\n",
    "              , \"\\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;\"\\\n",
    "              %(clf1.score(X_train, y_train).round(4)\\\n",
    "              , clf1.score(X_dev, y_dev).round(4)\\\n",
    "              , clf1.score(X_test, y_test).round(4)))\n",
    "    if svm_score > max_acc_val:\n",
    "        max_acc_val = svm_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nSVM:\" , max_mc\\\n",
    "              , \"\\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "              %(clf2.score(X_train, y_train).round(4)\\\n",
    "                , clf2.score(X_dev, y_dev).round(4)\\\n",
    "                , clf2.score(X_test, y_test).round(4)))\n",
    "    if mlp_score > max_acc_val:\n",
    "        max_acc_val = mlp_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nMLP:\", max_mc\\\n",
    "              , \" \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f;   \"\\\n",
    "              %(clf3.score(X_train, y_train).round(4)\\\n",
    "                , clf3.score(X_dev, y_dev).round(4)\\\n",
    "                , clf3.score(X_test, y_test).round(4)))\n",
    "    if knn_score > max_acc_val:\n",
    "        max_acc_val = knn_score\n",
    "        max_mc      = mc\n",
    "        print(\"\\nKNN:\", max_mc\\\n",
    "              , \" \\nTrain:      %.4f, \\nValidation: %.4f, \\nTest:       %.4f.   \"\\\n",
    "              %(clf4.score(X_train, y_train).round(4)\\\n",
    "              , clf4.score(X_dev, y_dev).round(4)\\\n",
    "              , clf4.score(X_test, y_test).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9783653846153846"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-tf2]",
   "language": "python",
   "name": "conda-env-nlp-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
